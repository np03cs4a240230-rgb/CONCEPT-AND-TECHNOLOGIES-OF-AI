{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Implementation from Scratch Step - by - Step Guide:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**3.1.1 Step -1- Data Understanding, Analysis and Preparations:**\n",
        "\n",
        "**• To - Do - 1:**\n",
        "1. Read and Observe the Dataset.\n",
        "2. Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n",
        "3. Print the Information of Datasets. {Hint: pd.info}.\n",
        "4. Gather the Descriptive info about the Dataset. {Hint: pd.describe}\n",
        "5. Split your data into Feature (X) and Label (Y).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gvtA0WWr1Pay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-4Coh6r1MJ3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Concepts and technologies of AI/student.csv\")\n",
        "\n",
        "# Print the first 5 rows\n",
        "print(\"Top 5 rows:\\n\", df.head())\n",
        "\n",
        "# Print the last 5 rows\n",
        "print(\"\\nBottom 5 rows:\\n\", df.tail())\n",
        "\n",
        "#print the information of data\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "#description info of java\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "#split data into x and y\n",
        "\n",
        "X = df[['Math', 'Reading']]\n",
        "Y = df['Writing']\n",
        "\n",
        "print(\"\\nFeatures (X):\\n\", X.head())\n",
        "print(\"\\nLabel (Y):\\n\", Y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0TulP3x2YVm",
        "outputId": "9c657ea8-16ba-492c-889a-db23f55a08fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows:\n",
            "    Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows:\n",
            "      Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n",
            "\n",
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n",
            "\n",
            "Features (X):\n",
            "    Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "\n",
            "Label (Y):\n",
            " 0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• To - Do - 2:**\n",
        "1. To make the task easier - let’s assume there is no bias or intercept.\n",
        "2. Create the matrices given in the question.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZViZcH2e2lpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features (Math, Reading)\n",
        "X = df[['Math', 'Reading']].values.T   # Transpose to get shape (d x n)\n",
        "\n",
        "# Weight vector (initialize with zeros)\n",
        "W = np.zeros((X.shape[0], 1))          # Shape (d x 1)\n",
        "\n",
        "# Labels (Writing marks)\n",
        "Y = df['Writing'].values.reshape(1, -1)  # Shape (1 x n)\n",
        "\n",
        "# Prediction using matrix multiplication\n",
        "Y_pred = W.T @ X  # Y = W^T * X\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"W shape:\", W.shape)\n",
        "print(\"Y shape:\", Y.shape)\n",
        "print(\"Y_pred shape:\", Y_pred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7dc1q1U2xz0",
        "outputId": "4f5bcde5-0092-485f-c2d4-4ba266c947df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (2, 1000)\n",
            "W shape: (2, 1)\n",
            "Y shape: (1, 1000)\n",
            "Y_pred shape: (1, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• To - Do - 3:**\n",
        "1. Split the dataset into training and test sets.\n",
        "2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest\n",
        "for testing.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FP3Nxkny3taa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "TCuQsIt43xCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features and label\n",
        "X = df[['Math', 'Reading']].values    # Feature matrix (n x d)\n",
        "Y = df['Writing'].values              # Label vector (n,)\n",
        "\n",
        "# Split dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"Y_train shape:\", Y_train.shape)\n",
        "print(\"Y_test shape:\", Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkg_jASH39RX",
        "outputId": "d7f095a3-a1fd-4043-bc24-00730cb914cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (800, 2)\n",
            "X_test shape: (200, 2)\n",
            "Y_train shape: (800,)\n",
            "Y_test shape: (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.2 Step -2- Build a Cost Function:**\n",
        "\n",
        "---\n",
        "**To - Do - 4:**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wGK-gjgl4C3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function (Mean Squared Error)\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X : Feature matrix (d x n)\n",
        "    Y : Target matrix (1 x n)\n",
        "    W : Weight matrix (d x 1)\n",
        "\n",
        "    Returns:\n",
        "    cost : Mean Squared Error (scalar)\n",
        "    \"\"\"\n",
        "    n = X.shape[1]               # number of samples\n",
        "    Y_pred = W.T @ X             # Prediction: Y_pred = W^T X\n",
        "    error = Y_pred - Y           # Difference between predicted and actual\n",
        "    cost = (1 / (2 * n)) * np.sum(error ** 2)  # MSE formula\n",
        "    return cost\n",
        "\n",
        "# Example usage:\n",
        "# cost = cost_function(X_matrix, Y_matrix, W)\n",
        "# print(\"Initial Cost:\", cost)"
      ],
      "metadata": {
        "id": "ZSuoqSHz4JpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To - Do - 5:**\n",
        "\n",
        "Make sure your code at To - Do - 4 passed the following test case:\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MGck9qmN4cLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test case\n",
        "X_test = np.array([[1, 3, 5],\n",
        "                   [2, 4, 6]])   # Feature matrix (2 features, 3 samples)\n",
        "\n",
        "Y_test = np.array([[3, 7, 11]])   # True labels (1 x 3)\n",
        "\n",
        "W_test = np.array([[1],\n",
        "                   [1]])          # Weight vector (2 x 1)\n",
        "\n",
        "# Calculate cost\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "# Check output\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement cost function\")\n",
        "    print(\"Cost function output:\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk4E7sw74gQL",
        "outputId": "12ddfa19-9bac-48b3-f426-229844d33228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To - Do - 6:**\n",
        "\n",
        "Implement your code for Gradient Descent; Either fill the following code or write your own:\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BY6FAf-K46Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (d x n).\n",
        "    Y (numpy.ndarray): Target vector (1 x n).\n",
        "    W (numpy.ndarray): Initial guess for parameters (d x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple: (W_update, cost_history)\n",
        "        W_update (numpy.ndarray): Updated parameters (d x 1).\n",
        "        cost_history (list): Cost values over iterations.\n",
        "    \"\"\"\n",
        "    cost_history = [0] * iterations\n",
        "    n = X.shape[1]  # number of samples\n",
        "\n",
        "    W_update = W.copy()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis values\n",
        "        Y_pred = W_update.T @ X  # shape (1 x n)\n",
        "\n",
        "        # Step 2: Difference between hypothesis and actual\n",
        "        loss = Y_pred - Y        # shape (1 x n)\n",
        "\n",
        "        # Step 3: Gradient calculation\n",
        "        dw = (1 / n) * (X @ loss.T)  # shape (d x 1)\n",
        "\n",
        "        # Step 4: Update weights\n",
        "        W_update = W_update - alpha * dw\n",
        "\n",
        "        # Step 5: Compute new cost\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W_update, cost_history"
      ],
      "metadata": {
        "id": "YVUeRms85Elv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To - Do - 7:**\n",
        "\n",
        "Make sure following Test Case is passe by your code from To - Do - 6 or your Gradient Descent\n",
        "Implementation:\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tZ7kti-H5cmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random test data\n",
        "np.random.seed(0)\n",
        "X_test = np.random.rand(3, 100)       # 3 features x 100 samples\n",
        "Y_test = np.random.rand(1, 100)       # Labels\n",
        "W_test = np.random.rand(3, 1)         # Initial weights\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Run Gradient Descent\n",
        "final_params, cost_history = gradient_descent(X_test, Y_test, W_test, alpha, iterations)\n",
        "\n",
        "# Print results\n",
        "print(\"Final Parameters:\\n\", final_params)\n",
        "print(\"First 10 Cost Values:\\n\", cost_history[:10])\n",
        "print(\"Last Cost Value:\", cost_history[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v7WtoAt5gAt",
        "outputId": "5ed57f64-5b51-48b9-b7bc-e2a5a91207e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters:\n",
            " [[0.27126876]\n",
            " [0.47953137]\n",
            " [0.08853683]]\n",
            "First 10 Cost Values:\n",
            " [np.float64(0.11501437366403591), np.float64(0.11417721076243244), np.float64(0.11335358952199538), np.float64(0.11254328596468793), np.float64(0.11174607982644355), np.float64(0.110961754495564), np.float64(0.11019009695213929), np.float64(0.10943089770847206), np.float64(0.10868395075049048), np.float64(0.10794905348013303)]\n",
            "Last Cost Value: 0.05532065979560406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To - Do - 8:**\n",
        "\n",
        "Implementation of RMSE in the Code - Complete the following code or write your own:\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YX413TaJ5z_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the Root Mean Squares.\n",
        "\n",
        "    Input Arguments:\n",
        "    Y: Array of actual (Target) Dependent Variables.\n",
        "    Y_pred: Array of predicted Dependent Variables.\n",
        "\n",
        "    Output Arguments:\n",
        "    rmse: Root Mean Square.\n",
        "    \"\"\"\n",
        "    n = Y.shape[1] if Y.ndim > 1 else len(Y)\n",
        "    rmse = np.sqrt(np.sum((Y_pred - Y) ** 2) / n)\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "MgztqWOO54Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To - Do - 9 - Implementation in the Code:**\n",
        "\n",
        "Complete the following code or write your own for r2 loss:\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pBl5SLEc6YJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - R2\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the R Squared Error.\n",
        "\n",
        "    Input Arguments:\n",
        "    Y: Array of actual (Target) Dependent Variables.\n",
        "    Y_pred: Array of predicted Dependent Variables.\n",
        "\n",
        "    Output Arguments:\n",
        "    r2: R Squared Error.\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)      # Total Sum of Squares\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)      # Sum of Squared Residuals\n",
        "    r2 = 1 - (ss_res / ss_tot)              # R-squared\n",
        "    return r2"
      ],
      "metadata": {
        "id": "lWWtDA5v6eqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• To - Do - 10:**\n",
        "\n",
        "We will define a function that:\n",
        "1. Loads the data and splits it into training and test sets.\n",
        "2. Prepares the feature matrix (X) and target vector (Y).\n",
        "3. Defines the weight matrix (W) and initializes the learning rate and number of iterations.\n",
        "4. Calls the gradient descent function to learn the parameters.\n",
        "5. Evaluates the model using RMSE and R2\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AB8Sqnon7TR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load dataset\n",
        "    data = pd.read_csv('/content/drive/MyDrive/Concepts and technologies of AI/student.csv')\n",
        "\n",
        "    # Features and target\n",
        "    X = data[['Math', 'Reading']].values.T\n",
        "    Y = data['Writing'].values.reshape(1, -1)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X.T, Y.T, test_size=0.2, random_state=42\n",
        "    )\n",
        "    X_train, X_test = X_train.T, X_test.T\n",
        "    Y_train, Y_test = Y_train.T, Y_test.T\n",
        "\n",
        "    # Initialize weights and hyperparameters\n",
        "    W = np.zeros((X_train.shape[0], 1))\n",
        "    alpha = 0.00001\n",
        "    iterations = 1000\n",
        "\n",
        "    # Train model\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Predict and evaluate\n",
        "    Y_pred = W_optimal.T @ X_test\n",
        "    print(\"Final Weights:\\n\", W_optimal)\n",
        "    print(\"Cost History (First 10):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", rmse(Y_test, Y_pred))\n",
        "    print(\"R-Squared on Test Set:\", r2(Y_test, Y_pred))\n",
        "\n",
        "# Execute main\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddr1G1zP7Wav",
        "outputId": "f21910c0-c5f3-4115-9927-1b5eef1e85b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights:\n",
            " [[0.34811659]\n",
            " [0.64614558]]\n",
            "Cost History (First 10): [np.float64(2013.165570783755), np.float64(1640.286832599692), np.float64(1337.0619994901588), np.float64(1090.479489285058), np.float64(889.9583270083237), np.float64(726.8940993009545), np.float64(594.2897260808595), np.float64(486.4552052951634), np.float64(398.7634463599483), np.float64(327.45171473246876)]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To - Do - 11 - Present your finding:**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**1. Did your Model Overfitt, Underfitts, or performance is acceptable.**\n",
        "\n",
        "->*The model’s performance is acceptable. The test RMSE is reasonable, and the R² value is high (≈0.89), which means the model explains most of the variation in writing scores. There are no signs of overfitting or underfitting; the model generalizes well to unseen data.*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**2. Experiment with different value of learning rate, making it higher and lower, observe the result.**\n",
        "\n",
        "->*I experimented with different learning rates:*\n",
        "\n",
        "* *Current learning rate (α = 0.00001): The model converges smoothly, but slowly.*\n",
        "\n",
        "* *Higher learning rate (e.g., 0.0001 or 0.001): The model learns faster, but if too high, the cost starts to fluctuate or even diverge.*\n",
        "\n",
        "* *Lower learning rate (e.g., 0.000001): The model is very stable, but convergence is much slower and requires more iterations.*\n",
        "\n",
        "**Observation:** *A moderate learning rate gives the best balance between speed and stability.*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xGQvd8zn8XWt"
      }
    }
  ]
}